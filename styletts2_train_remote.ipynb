{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset notes\n",
    "Training will fail with samples that are too short (<1 second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download necessary files and set base config\n",
    "Specify the experiment name in `EXPERIMENT_NAME` and download the dataset and OOD text list. This will also configure some default settings in `Configs/config.yml`.\n",
    "\n",
    "`DATASET_URL` should point to a google drive hosted .zip containing:\n",
    "\n",
    "* train_list.txt\n",
    "* val_list.txt\n",
    "* *.wav\n",
    "\n",
    "`OOD_URL` should point to an OOD text list (the corresponding audio files are not needed; this is just used to test the TTS model on out-of-dataset text)\n",
    "\n",
    "`PRETRAINED`, if populated, should point to a direct download for a first-stage checkpoint or pretrained second-stage model (depending on what stage of training you plan to do.)\n",
    "\n",
    "It is recommended to use huggingface if your home internet bandwidth is not fantastic.\n",
    "If it is great, you can use `vastai copy` to copy files to the desired locations. These are the files you need to provide:\n",
    "* `/root/StyleTTS2/Data/train_list.txt`\n",
    "* `/root/StyleTTS2/Data/val_list.txt`\n",
    "* `/root/StyleTTS2/{EXPERIMENT_NAME}_data/*.wav`\n",
    "* `/root/StyleTTS2/Models/{EXPERIMENT_NAME}/pretrained.pth` (if using a pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"Twilight0\"\n",
    "DATASET_URL = \"https://huggingface.co/datasets/therealvul/StyleTTS2MLP/resolve/main/StyleTTS2Twilight.zip?download=true\"\n",
    "OOD_URL = \"https://gist.githubusercontent.com/effusiveperiscope/f3a6d48ad3463b63d1cb9a53bfab9dd9/raw/480f168c0f4cdc96c8863ac09c84978b364ee6a1/gistfile1.txt\"\n",
    "PRETRAINED_URL = \"https://huggingface.co/therealvul/StyleTTS2/resolve/main/Unfinished/epoch_1st_00051.pth?download=true\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset\n",
      "Downloaded dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:   0%|          | 2/6199 [00:00<05:22, 19.23file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading OOD\n",
      "Downloading https://gist.githubusercontent.com/effusiveperiscope/f3a6d48ad3463b63d1cb9a53bfab9dd9/raw/480f168c0f4cdc96c8863ac09c84978b364ee6a1/gistfile1.txt to D:/styletts2test\\Data\\OOD_texts.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58.4k/58.4k [00:00<00:00, 8.07MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded OOD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "import gdown\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Download and extract dataset and data labels\n",
    "styletts_basedir = \"/root/StyleTTS2\" \n",
    "dataset_file = os.path.join(styletts_basedir, f\"dataset_{EXPERIMENT_NAME}.zip\")\n",
    "dataset_target_dir = os.path.join(styletts_basedir, f\"{EXPERIMENT_NAME}_data\")\n",
    "model_dir = os.path.join(styletts_basedir, \"Models\", EXPERIMENT_NAME)\n",
    "\n",
    "def download_file(url, dest):\n",
    "    print(f\"Downloading {url} to {dest}\")\n",
    "    with urllib.request.urlopen(url) as r, open(dest, 'wb') as out_file:\n",
    "        total_size = int(r.info().get('Content-Length', 0))\n",
    "        block_size = 1024\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "            while True:\n",
    "                data = r.read(block_size)\n",
    "                if not data:\n",
    "                    break\n",
    "                out_file.write(data)\n",
    "                pbar.update(len(data))\n",
    "\n",
    "print(\"Downloading dataset\")\n",
    "if not os.path.exists(dataset_file):\n",
    "    if \"drive.google.com\" in DATASET_URL:\n",
    "        gdown.download(DATASET_URL, output=dataset_file, quiet=False)\n",
    "    else:\n",
    "        download_file(DATASET_URL, dataset_file)\n",
    "print(\"Downloaded dataset\")\n",
    "\n",
    "assert(os.path.exists(dataset_file))\n",
    "\n",
    "if not os.path.exists(dataset_target_dir):\n",
    "    os.makedirs(dataset_target_dir, exist_ok=True)\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(dataset_file, 'r') as f:\n",
    "    files_count = len(f.namelist())\n",
    "    with tqdm(total=files_count, desc=f\"Extracting data\",\n",
    "        unit=\"file\") as pbar:\n",
    "        for info in f.namelist():\n",
    "            if info.endswith('.txt'):\n",
    "                unzip_file_path = os.path.join(styletts_basedir, 'Data')\n",
    "                f.extract(info, unzip_file_path) # Not sure if this can overwrite data?\n",
    "            elif info.endswith('.wav'):\n",
    "                unzip_file_path = dataset_target_dir\n",
    "                if os.path.exists(os.path.join(unzip_file_path, info)):\n",
    "                    continue\n",
    "                f.extract(info, unzip_file_path)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Download OOD text\n",
    "ood_out_path = os.path.join(styletts_basedir, 'Data', 'OOD_texts.txt')\n",
    "if not os.path.exists(ood_out_path):\n",
    "    print(\"Downloading OOD\")\n",
    "    download_file(OOD_URL, ood_out_path)\n",
    "    print(\"Downloaded OOD\")\n",
    "\n",
    "# Download pretrained model\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "if PRETRAINED_URL is not None and not os.path.exists(os.path.join(model_dir, 'pretrained.pth')):\n",
    "    print(\"Downloading pretrained model\")\n",
    "    download_file(PRETRAINED_URL, os.path.join(model_dir, 'pretrained.pth'))\n",
    "    print(\"Downloaded pretrained model\")\n",
    "\n",
    "# Setup base config\n",
    "import yaml\n",
    "config_path = os.path.join(styletts_basedir, 'Configs', 'config.yml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config['log_dir'] = f'Models/{EXPERIMENT_NAME}'\n",
    "    if PRETRAINED_URL is not None:\n",
    "        config['pretrained_model'] = os.path.join(model_dir, \"pretrained.pth\")\n",
    "    config['data_params']['root_path'] = dataset_target_dir\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First stage (acoustic reconstruction training)\n",
    "This is only for training a model from scratch; you do not need to run this step if you only plan to finetune the diffusion (second stage) model.\n",
    "\n",
    "Configure the settings in the cell before running.\n",
    "\n",
    "* `TOTAL_EPOCHS` - total epochs used for first stage training. The paper used `100` epochs for LJSpeech (single speaker, 24hrs), `50` epochs for VCTK (109 speakers, 44hrs), and `30` epochs for LibriTTS (1151 speakers, 245hrs).\n",
    "* `TMA_START_POINT` - epochs before which transferable monotonic alignment is trained. Epochs in the TMA training regime are typically 20x longer than epochs before--so this should be expected to take up a large portion of training time. [The author used TMA_epoch = 5 for LibriTTS](https://github.com/yl4579/StyleTTS2/issues/18). I found `50` to be a reasonable value for a 5-hour dataset, so this can probably be lowered for larger datasets.\n",
    "* `LOAD_ONLY_PARAMS` - Whether only parameters are loaded (vs. loading optimizer state and epoch information). If you are continuing the training process of an existing first-stage checkpoint, this should be `False`.\n",
    "* `USE_CHECKPOINT` - Manually specify a checkpoint path to use. This may be useful, for example, if you needed to interrupt training to modify training parameters.\n",
    "* `SAVE_FREQ` - Determines how often epochs are saved. Be mindful of your allocated disk limits. Because TMA epochs take so long to complete, you may want to consider lowering this down to 1 during TMA training.\n",
    "* `BATCH_SIZE` - Higher values will increase training speed but require more VRAM. A batch size of 2 will fit on a 16GB GPU during both training stages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_EPOCHS = 100 \n",
    "TMA_START_POINT = 50 \n",
    "LOAD_ONLY_PARAMS = True\n",
    "USE_CHECKPOINT = None\n",
    "SAVE_FREQ = 2\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "styletts_basedir = styletts_basedir or \"/root/StyleTTS2\"\n",
    "config_path = config_path or os.path.join(styletts_basedir, 'Configs', 'config.yml')\n",
    "pretrained_model = os.path.join(styletts_basedir, \"Models\", EXPERIMENT_NAME, \"pretrained.pth\")\n",
    "\n",
    "import os\n",
    "os.chdir(styletts_basedir)\n",
    "\n",
    "import yaml\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config['epochs_1st'] = TOTAL_EPOCHS\n",
    "    config['loss_params']['TMA_epoch'] = TMA_START_POINT\n",
    "    config['load_only_params'] = LOAD_ONLY_PARAMS\n",
    "    if (USE_CHECKPOINT is not None):\n",
    "        config['pretrained_model'] = USE_CHECKPOINT\n",
    "    elif os.path.exists(pretrained_model):\n",
    "        config['pretrained_model'] = pretrained_model\n",
    "    else:\n",
    "        config['pretrained_model'] = \"\"\n",
    "    config['save_freq'] = SAVE_FREQ\n",
    "    config['batch_size'] = BATCH_SIZE\n",
    "    config['second_stage_load_pretrained'] = False\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "!accelerate launch --mixed_precision=fp16 train_first.py --config_path ./Configs/config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second stage (diffusion training/finetuning)\n",
    "[This discussion page](https://github.com/yl4579/StyleTTS2/discussions/81)\n",
    "contains some useful information on finetuning. Some gotchas:\n",
    "* It is basically impossible to finetune on a 16GB GPU--you need 24GB minimum. The original models were trained on 4xA100 (e.g. 160 GB VRAM).\n",
    "* Epochs are zero-indexed in the second stage. This includes `DIFF_BEGIN_EPOCH` and `JOINT_EPOCH`.\n",
    "\n",
    "Configure the settings in the below cell before running.\n",
    "* `IS_FIRST_STAGE` - **!!! Set this if the checkpoint provided is a first stage checkpoint !!!**.\n",
    "* `MAX_LEN` - the maximum length of training samples, in frames (.0125 second/frame; 100 frames = 1.25 seconds, 800 frames = 10 seconds). This will affect VRAM usage and the ability of the model to coherently synthesize longer samples. A `MAX_LEN` below 175 is not recommended. Do not reduce `MAX_LEN` below 100.\n",
    "* `TOTAL_EPOCHS` - total epochs used for second stage training. The paper used 60 epochs on LJSpeech, 40 epochs on VCTK, and 25 epochs on LibriTTS.\n",
    "* `DIFF_BEGIN_EPOCH` - The epoch at which style diffusion training begins. This can be as low as `1` (i.e. second epoch) for large datasets. If you set this above `TOTAL_EPOCHS`, you can disable style vector diffusion, lowering VRAM usage at the cost of output quality. \n",
    "* `JOINT_BEGIN_EPOCH` - The epoch at which SLM adversarial training begins. This cannot be set lower than `DIFF_BEGIN_EPOCH` or an error will occur (SLM adversarial cannot be run without diffusion). SLM adversarial training will increase VRAM usage. A model can be produced without adversarial training, at the expense of output quality.\n",
    "* `BATCH_SIZE` - Higher values will increase training speed but require more VRAM. **Will break if lowered below (NUM_GPUS) * 2.**\n",
    "* `USE_CHECKPOINT` - Manually specify a checkpoint path to use. *This may be useful for the technique described in the above discussion where you can produce a diffusion-trained-only checkpoint at a higher batch size, then shift to a lower batch size for SLM training.*\n",
    "* `SAVE_FREQ` - Determines how often epochs are saved. Be mindful of your allocated disk limits. Because TMA epochs take so long to complete, you may want to consider lowering this down to 1 during TMA training.\n",
    "* `LOAD_ONLY_PARAMS` - Whether only parameters are loaded (vs. loading optimizer state and epoch information). If you are starting a fresh finetune from a pretrained **2nd stage** checkpoint, set to `True`. If you are starting a fresh finetune from a pretrained 1st stage check point, or if you are continuing the training process of a second-stage checkpoint, set to `False`.\n",
    "* `DISABLE_AUTORESUME_BEHAVIOR` - **For my ease of use, this cell will automatically load the most recent 2nd stage checkpoint if found in the model folder.** This helps with stopping and starting training at different batch sizes for diffusion or slmadv. Setting this to `True` will disable this behavior and allow you to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_FIRST_STAGE = False\n",
    "MAX_LEN = 175\n",
    "TOTAL_EPOCHS = 5\n",
    "DIFF_BEGIN_EPOCH = 1\n",
    "JOINT_BEGIN_EPOCH = 5\n",
    "BATCH_SIZE = 4\n",
    "SEGMENTED_BATCH_SIZE = [16, 8, 6] # works on 45GB\n",
    "USE_CHECKPOINT = None\n",
    "SAVE_FREQ = 2\n",
    "LOAD_ONLY_PARAMS = False and not IS_FIRST_STAGE\n",
    "DISABLE_AUTORESUME_BEHAVIOR = False\n",
    "\n",
    "styletts_basedir = styletts_basedir or \"/root/StyleTTS2\"\n",
    "config_path = config_path or os.path.join(styletts_basedir, 'Configs', 'config.yml')\n",
    "pretrained_model = os.path.join(styletts_basedir, \"Models\", EXPERIMENT_NAME, \"pretrained.pth\")\n",
    "\n",
    "import re\n",
    "if (not DISABLE_AUTORESUME_BEHAVIOR):\n",
    "    fs = os.listdir(f'/root/StyleTTS2/Models/{EXPERIMENT_NAME}')\n",
    "    pretrained_matches = sorted(\n",
    "        filter(lambda x: x.startswith(\"epoch_2nd_\"), fs),\n",
    "        key=lambda x: int(re.search(r'epoch_2nd_(\\d+).pth', x).group(1)))\n",
    "    if len(pretrained_matches):\n",
    "        found_model = pretrained_matches[-1]\n",
    "        print(f\"========== USING FOUND 2ND STAGE CKPT: {found_model} ====================\")\n",
    "        pretrained_model = found_model\n",
    "        USE_CHECKPOINT = found_model\n",
    "\n",
    "import os\n",
    "os.chdir(styletts_basedir)\n",
    "\n",
    "import yaml\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config['max_len'] = MAX_LEN\n",
    "    config['epochs_2nd'] = TOTAL_EPOCHS\n",
    "    config['loss_params']['diff_epoch'] = DIFF_BEGIN_EPOCH\n",
    "    config['loss_params']['joint_epoch'] = JOINT_BEGIN_EPOCH\n",
    "    config['batch_size'] = BATCH_SIZE\n",
    "    config['segmented_batch_size'] = SEGMENTED_BATCH_SIZE\n",
    "    config['second_stage_load_pretrained'] = not IS_FIRST_STAGE\n",
    "    if (USE_CHECKPOINT is not None):\n",
    "        # If IS_FIRST_STAGE is false this is ignored anyways\n",
    "        config['first_stage_path'] = pretrained_model \n",
    "        config['pretrained_model'] = USE_CHECKPOINT\n",
    "    elif os.path.exists(pretrained_model):\n",
    "        config['first_stage_path'] = pretrained_model\n",
    "        config['pretrained_model'] = pretrained_model\n",
    "    else:\n",
    "        config['first_stage_path'] = \"\"\n",
    "        config['pretrained_model'] = \"\"\n",
    "    config['save_freq'] = SAVE_FREQ\n",
    "    config['load_only_params'] = LOAD_ONLY_PARAMS\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "#!accelerate launch --mixed_precision=fp16 train_second.py --config_path ./Configs/config.yml\n",
    "!accelerate launch --mixed_precision=fp16 train_second_segmented.py --config_path ./Configs/config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tensorboard logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir os.path.join(styletts_basedir, \"Models\", EXPERIMENT_NAME, \"tensorboard\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
