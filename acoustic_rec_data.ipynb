{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models/Multi0_40/epoch_2nd_40_1c872.pth\n",
      "bert loaded\n",
      "bert_encoder loaded\n",
      "predictor loaded\n",
      "decoder loaded\n",
      "text_encoder loaded\n",
      "predictor_encoder loaded\n",
      "style_encoder loaded\n",
      "diffusion loaded\n",
      "text_aligner loaded\n",
      "pitch_extractor loaded\n",
      "mpd loaded\n",
      "msd loaded\n",
      "wd loaded\n",
      "en 1.0179164409637451 F0_real 552.5 real_norm 7.289944648742676 s 1.0576171875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\StyleTTS2\\Modules\\istftnet.py:101: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at ..\\aten\\src\\ATen\\EmptyTensor.cpp:32.)\n",
      "  magnitude * torch.exp(phase * 1j),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan check decoder: False\n",
      "(37800,)\n",
      "0.21805707\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.dtype' object has no attribute 'kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 191\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(yr\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(yr\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m--> 191\u001b[0m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_OUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtru_ard_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi2\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m48000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw48k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m write(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_OUTPUT_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred_ard_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m), sr, yr)\n",
      "File \u001b[1;32mc:\\Users\\vul\\miniconda3\\envs\\styletts2\\Lib\\site-packages\\scipy\\io\\wavfile.py:772\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(filename, rate, data)\u001b[0m\n\u001b[0;32m    769\u001b[0m fs \u001b[38;5;241m=\u001b[39m rate\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 772\u001b[0m     dkind \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkind\u001b[49m\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (dkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (dkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    774\u001b[0m                                              data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    775\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported data type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m data\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'torch.dtype' object has no attribute 'kind'"
     ]
    }
   ],
   "source": [
    "# Generating acoustic reconstruction data for postprocessing stages\n",
    "CONFIG_PATH = \"Models/Multi0_40/config.yml\"\n",
    "CKPT_PATH = \"Models/Multi0_40/epoch_2nd_40_1c872.pth\"\n",
    "DIR_48KS = \"Z:/\"\n",
    "DATA_OUTPUT_DIR = \"acoustic_rec_data\"\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "from meldataset import build_dataloader_with_ref_48k\n",
    "from models import *\n",
    "from utils import *\n",
    "from losses import *\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "from itertools import chain\n",
    "from scipy.io.wavfile import write\n",
    "from accelerate import Accelerator, DistributedDataParallelKwargs\n",
    "\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dp = config['data_params']\n",
    "sr = config['preprocess_params'].get('sr', 24000)\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "log_dir = config['log_dir']\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_list, val_list = get_data_path_list(\n",
    "    dp['train_data'], dp['val_data'])\n",
    "val_path = dp['val_data']\n",
    "root_path = dp['root_path']\n",
    "min_length = dp['min_length']\n",
    "OOD_data = dp['OOD_data']\n",
    "max_len = 800\n",
    "\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "accelerator = Accelerator(project_dir=log_dir,\n",
    "    kwargs_handlers=[ddp_kwargs], mixed_precision='fp16')\n",
    "device = accelerator.device\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get('ASR_config', False)\n",
    "ASR_path = config.get('ASR_path', False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get('F0_path', False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "BERT_path = config.get('PLBERT_dir', False)\n",
    "plbert = load_plbert(BERT_path)\n",
    "\n",
    "train_dataloader = build_dataloader_with_ref_48k(\n",
    "    train_list, root_path, dir_48ks = DIR_48KS, OOD_data=OOD_data,\n",
    "    min_length=min_length, batch_size=batch_size, num_workers=2,\n",
    "    dataset_config={}, device=device)\n",
    "val_dataloader = build_dataloader_with_ref_48k(\n",
    "    val_list, root_path, dir_48ks = DIR_48KS, OOD_data=OOD_data,\n",
    "    min_length=min_length, batch_size=batch_size, validation=True,\n",
    "    num_workers=0, device=device, dataset_config={})\n",
    "\n",
    "import logging\n",
    "model_params = recursive_munch(config['model_params'])\n",
    "multispeaker = model_params.multispeaker\n",
    "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "\n",
    "for k in model:\n",
    "    model[k] = accelerator.prepare(model[k])\n",
    "    model[k].eval()\n",
    "\n",
    "try:\n",
    "    n_down = model.text_aligner.module.n_down\n",
    "except:\n",
    "    n_down = model.text_aligner.n_down\n",
    "\n",
    "print(f\"Loading {CKPT_PATH}\")\n",
    "params_whole = torch.load(CKPT_PATH, map_location='cpu')\n",
    "params = params_whole['net']\n",
    "\n",
    "for key in model:\n",
    "    if key in params:\n",
    "        print('%s loaded' % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:] # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            model[key].load_state_dict(new_state_dict, strict=False)\n",
    "#             except:\n",
    "#                 _load(params[key], model[key])\n",
    "_ = [model[key].eval() for key in model]\n",
    "\n",
    "#print(max_len)\n",
    "#print(model.diffusion.diffusion.sigma_data)\n",
    "\n",
    "if not os.path.exists(DATA_OUTPUT_DIR):\n",
    "    os.makedirs(DATA_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "for i, batch in enumerate(chain(val_dataloader, train_dataloader)):\n",
    "    with torch.no_grad():\n",
    "        waves = batch[0]\n",
    "        waves_48k = batch[-1]\n",
    "        batch = [b.to(device) for b in batch[1:-1]]\n",
    "        texts, input_lengths, _, _, mels, mel_input_length, _ = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mask = length_to_mask(mel_input_length // (2 ** n_down)).to('cuda')\n",
    "            ppgs, s2s_pred, s2s_attn = model.text_aligner(mels, mask, texts)\n",
    "\n",
    "            s2s_attn = s2s_attn.transpose(-1, -2)\n",
    "            s2s_attn = s2s_attn[..., 1:]\n",
    "            s2s_attn = s2s_attn.transpose(-1, -2)\n",
    "\n",
    "            text_mask = length_to_mask(input_lengths).to(texts.device)\n",
    "            attn_mask = (~mask).unsqueeze(-1).expand(mask.shape[0], mask.shape[1], text_mask.shape[-1]).float().transpose(-1, -2)\n",
    "            attn_mask = attn_mask.float() * (~text_mask).unsqueeze(-1).expand(text_mask.shape[0], text_mask.shape[1], mask.shape[-1]).float()\n",
    "            attn_mask = (attn_mask < 1)\n",
    "            s2s_attn.masked_fill_(attn_mask, 0.0)\n",
    "\n",
    "        # encode\n",
    "        t_en = model.text_encoder(texts, input_lengths, text_mask)\n",
    "        \n",
    "        asr = (t_en @ s2s_attn)\n",
    "\n",
    "        # get clips\n",
    "        #mel_input_length_all = accelerator.gather(mel_input_length) # for balanced load\n",
    "        #mel_len = min([int(mel_input_length.min().item() / 2 - 1), max_len // 2])\n",
    "        mel_input_length_all = accelerator.gather(mel_input_length) # for balanced load\n",
    "        mel_len = min([int(mel_input_length_all.min().item() / 2 - 1), max_len // 2])\n",
    "        mel_len_st = int(mel_input_length_all.min().item() / 2 - 1)\n",
    "        \n",
    "        en = []\n",
    "        gt = []\n",
    "        wav = []\n",
    "        wav48k = []\n",
    "        st = []\n",
    "\n",
    "        for bib in range(len(mel_input_length)):\n",
    "            mel_length = int(mel_input_length[bib].item() / 2)\n",
    "\n",
    "            random_start = np.random.randint(0, mel_length - mel_len)\n",
    "            en.append(asr[bib, :, random_start:random_start+mel_len])\n",
    "            gt.append(mels[bib, :, (random_start * 2):((random_start+mel_len) * 2)])\n",
    "            y = waves[bib][(random_start * 2) * 300:((random_start+mel_len) * 2) * 300]\n",
    "            y48k = waves_48k[bib][\n",
    "                (random_start * 2) * 600:((random_start+mel_len) * 2) * 600]\n",
    "\n",
    "            wav.append(torch.from_numpy(y).to('cuda'))\n",
    "            wav48k.append(torch.from_numpy(y48k).to('cuda'))\n",
    "\n",
    "            random_start = np.random.randint(0, mel_length - mel_len_st)\n",
    "            st.append(mels[bib, :, (random_start * 2):((random_start+mel_len_st) * 2)])\n",
    "\n",
    "        wav = torch.stack(wav).float().detach()\n",
    "        wav48k = torch.stack(wav48k).float().detach()\n",
    "\n",
    "        en = torch.stack(en)\n",
    "        gt = torch.stack(gt).detach()\n",
    "        st = torch.stack(st).detach()\n",
    "\n",
    "        F0_real, _, _ = model.pitch_extractor(gt.unsqueeze(1))\n",
    "        # style encoder keeps outputting NaN?\n",
    "        # st is not nan but style_encoder is\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            s = model.style_encoder(st.unsqueeze(1) if multispeaker else gt.unsqueeze(1))\n",
    "        real_norm = log_norm(gt.unsqueeze(1)).squeeze(1)\n",
    "        #print(f\"nan checks: F0_real {F0_real.isnan().any()} real_norm {real_norm.isnan().any()} s {s.isnan().any()}\")\n",
    "        print(f\"en {en.max()} F0_real {F0_real.max()} real_norm {real_norm.max()} s {s.max()}\")\n",
    "        y_rec = model.decoder(en, F0_real, real_norm, s)\n",
    "        print(f\"nan check decoder: {y_rec.isnan().any()}\")\n",
    "        #print(y_rec.isnan().any())\n",
    "        # Why are these NaN?\n",
    "\n",
    "        y_rec = y_rec.squeeze(1)\n",
    "        y_rec = y_rec.cpu().numpy()\n",
    "        wav = wav.cpu().numpy()\n",
    "        wav48k = wav48k.cpu().numpy()\n",
    "\n",
    "        #print(wav.shape)\n",
    "        #print(y_rec.shape)\n",
    "        for i2 in range(wav.shape[0]):\n",
    "            w48k = wav48k[i2,:]\n",
    "            yr = y_rec[i2,:]\n",
    "            print(yr.shape)\n",
    "            print(yr.max())\n",
    "            write(os.path.join(DATA_OUTPUT_DIR, f\"gtru_ard_{i}_{i2}.wav\"), 48000, w48k)\n",
    "            write(os.path.join(DATA_OUTPUT_DIR, f\"pred_ard_{i}_{i2}.wav\"), sr, yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "styletts2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
